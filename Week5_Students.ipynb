{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# data generation\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import fbeta_score , accuracy_score , recall_score , precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "__Wisdom of crowds:__\n",
    "\n",
    "__Ensemble:__\n",
    "\n",
    "__Ensemble learning:__\n",
    "\n",
    "__Ensemble method:__\n",
    "\n",
    "__Weak learner:__\n",
    "\n",
    "__Strong learner:__\n",
    "\n",
    "__Hard voting:__\n",
    "\n",
    "__Soft voting:__\n",
    "\n",
    "__Bagging:__\n",
    "\n",
    "__Pasting:__\n",
    "\n",
    "__Boosting:__\n",
    "\n",
    "__out of bag instances:__\n",
    "\n",
    "__oob_score:__\n",
    "\n",
    "__Feature sampling:__\n",
    "\n",
    "__Random patches method:__\n",
    "\n",
    "__Feature importance:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"clean_charity_ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_train['income']\n",
    "X = data_train.drop(columns='income')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.mean(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeClassifier()\n",
    "dt_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = dt_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy and oob_score of the random forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the attributes of a Random Forrest and what do they mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Random Forrest with more and smaller trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's figure out the optimal number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_accuracy(predictors, X_test,  y_test):\n",
    "    \n",
    "    np.random.shuffle(predictors)\n",
    "    N = len(predictors)\n",
    "    summed_prediction = np.zeros(len(y_test))\n",
    "    acc_score = np.zeros(N)\n",
    "    \n",
    "    for i in range(N): \n",
    "        y_hat_new = predictors[i].predict(X_test)\n",
    "        summed_prediction += y_hat_new\n",
    "        averaged_prediction = summed_prediction/float(i+1)\n",
    "\n",
    "        acc_score[i] = accuracy_score(y_test, (averaged_prediction>0.5).astype(int))\n",
    "        \n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    plt.plot(cumulative_accuracy(rf_clf.estimators_, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figs/BaggingVsBoosting.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create an AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_clf = AdaBoostClassifier(learning_rate=0.8, n_estimators=100, algorithm='SAMME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### staged_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how do the feature impartances change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### different Ensemble Methods create different decison boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figs/DecisionBoundaries.PNG') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image from https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make our own decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blob_data(centers=[(0,0), (0,1), (1,0), (1,1)], sigma=0.3, plot=True):\n",
    "    X, y = make_blobs(centers=centers, cluster_std=sigma)\n",
    "    labels = (y%3!=0).astype(int)\n",
    "    x_min = min(X[:, 0])\n",
    "    x_max = max(X[:, 0])\n",
    "    y_min = min(X[:, 1])\n",
    "    y_max = max(X[:, 1])\n",
    "    if plot:\n",
    "        plt.scatter(X[:, 0], X[:,1], c=labels)\n",
    "        plt.xlabel('$X_1$', size=16)\n",
    "        plt.xlim(x_min-0.1*(x_max-x_min), x_max+0.1*(x_max-x_min))\n",
    "        plt.ylabel('$X_2$', size=16)\n",
    "        plt.ylim(y_min-0.1*(y_max-y_min), y_max+0.1*(y_max-y_min))\n",
    "    \n",
    "    features = pd.DataFrame({\"x1\":X[:,0], \"x2\":X[:,1]})\n",
    "    targets = pd.DataFrame({\"y\":labels})\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_beundary(features, targets, predictor, res=30):\n",
    "    x_min=features['x1'].min()\n",
    "    x_max=features['x1'].max()    \n",
    "    y_min=features['x2'].min()\n",
    "    y_max=features['x2'].max()\n",
    "    \n",
    "    xx = np.linspace(x_min, x_max, res)\n",
    "    yy = np.linspace(y_min,y_max, res)\n",
    "    xv, yv = np.meshgrid(xx,yy)\n",
    "    mesh = np.vstack((xv.flatten(), yv.flatten())).T\n",
    "    pred = predictor.predict(mesh)\n",
    "    \n",
    "    cs = plt.contourf(xv,yv, pred.reshape(xv.shape), cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.scatter(features['x1'], features['x2'], c=targets['y'])\n",
    "    plt.xlabel('$X_1$', size=16)\n",
    "    plt.xlim(x_min-0.1*(x_max-x_min), x_max+0.1*(x_max-x_min))\n",
    "    plt.ylabel('$X_2$', size=16)\n",
    "    plt.ylim(y_min-0.1*(y_max-y_min), y_max+0.1*(y_max-y_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets= create_blob_data(sigma=0.1)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_beundary(features, targets, clf, res=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Baggin vs Boosting\n",
    "\n",
    "There’s not an outright winner; it depends on the data, the simulation and the circumstances. Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability.\n",
    "\n",
    "If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.\n",
    "\n",
    "By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting.\n",
    "\n",
    "https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figs/GradientVsAda.PNG') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(N=100):\n",
    "    x= np.linspace(-1,1,N)\n",
    "    y = 2*(x**2)+np.random.normal(loc=0, scale=0.3, size=N)\n",
    "    return x.reshape(len(x),1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = make_data()\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingRegressor()\n",
    "gb_clf.fit(x,y)\n",
    "ests = gb_clf.estimators_\n",
    "for i in [0, 1, 10,  50, 99]:\n",
    "    plt.plot(x, ests[i][0].predict(x), linewidth=3, label=str(i))\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\n",
    "\n",
    "https://medium.com/datadriveninvestor/understanding-adaboost-and-scikit-learns-algorithm-c8d8af5ace10\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\n",
    "\n",
    "https://www.mygreatlearning.com/blog/gradient-boosting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
